---
title: "Simulation,Optimization"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, message = FALSE}
library(tidyverse)
library(here)
set.seed(36528)
```

```{r, include = FALSE}
true_pi <- 0.8
true_mu <- c(0,   2)
true_sigma <- c(1,   0.5)
n <- 250
pop <- rbinom(n, size = 1, prob = 1 - true_pi) + 1
x <- rnorm(n, mean = true_mu[pop], sd = true_sigma[pop])
```

# Optimization of Multivariate Function Using optim()

## Example: Two Normal Mixture 

Consider the following sample, $n = 250$

```{r}
ggplot(mapping = aes(x = x)) +
  geom_histogram(binwidth = 0.25)
```

One model might be a two component Normal mixture: arises when with probability $\pi$ we get a value from N$(\mu_1, \sigma_1^2)$, and otherwise a sample from N$(\mu_2, \sigma_2^2)$.

$$
f(x; \pi, \mu_1, \mu_2, \sigma_1, \sigma_2) = \pi\phi(x; \mu_1, \sigma_1^2) +  (1- \pi)\phi(x; \mu_2, \sigma_2^2) 
$$
where $\phi(x; \mu, \sigma^2)$ is the density function for a Normal($\mu, \sigma^2)$.

## Your Turn

Let $\theta = (\pi, \mu_1, \mu_2, \sigma_1, \sigma_2)$.

Fill in the body of the function to calculate the density function for a specified `theta` and data vector `x`:
```{r}
dmix <- function(x, theta){
  pi <- theta[1]
  mu_1 <- theta[2]
  mu_2 <- theta[3]
  sigma_1 <- theta[4]
  sigma_2 <- theta[5]
  pi * dnorm(x, mean = mu_1, sd = sigma_1) +
   (1 - pi) * dnorm(x, mean = mu_2, sd = sigma_2) 
} 
dmix(0, c(0, 1, 1, 1, 1))
```

Then try some more reasonable parameters to compare to the data:
```{r}
ggplot(mapping = aes(x = x)) +
  geom_histogram(aes(y = stat(density)), binwidth = 0.25) +
  stat_function(data = data.frame(), fun = dmix, args = list(theta = c(0.6, 0.2, 2, 1.5, 1)))
```


## Find MLE with maximization

Need negative log likelihood:
```{r}
nllhood_mix <- function(theta, x){
  d <- dmix(x, theta)
  -1 * sum(log(d))
}
```

`optim()` with `method = "BFGS"`, does a **Quasi** Newton method (swaps Hessian with an approximation).


Provide starting values to the `par` argument: values for $\theta$ we think are close to the values that give the minimum:

## Things that go wrong

* **Lot's of messages** - bad parameter values lead to messages with `NaN`
* **Non convergence** no convergence due to bad parameter values
* **Local minima** 

## Lot's of messages 

This converges, `$convergence` is `0`, but we get a lot of messages about `NaN`.  This happens when a call to `nllhood_mix()` returns `NaN`, mostly when using bad values for the parameters, e.g. $\pi = -1$, or $\sigma_1 < 0$.

```{r, warning = FALSE, message = FALSE}
(mle1 <- optim(par = c(1, 0, 1.8, 1.2, 0.7), fn = nllhood_mix, x = x, method = "BFGS"))
```


`optim()` can handle this fine, so we don't really need to worry.

To avoid `NaN` warnings we could protect our function from returning values at bad parameter values:
```{r}
nllhood_mix2 <- function(theta, x){
  d <- dmix(x, theta)
  if (any(d < 0) || any(is.nan(d))){
    return(Inf)
  }
  -1 * sum(log(d))
}
```

But this does introduce some weird discontinuities in our function.

```{r, warning = FALSE, message = FALSE}
optim(par = c(0.4, 2, -0.2, 1, 2), fn = nllhood_mix2, x = x, 
  method = "BFGS")
```

Or use a bounded algorithm (see below).

## Non-convergence

This doesn't converge, `$convergence` is `1`,:
```{r, warning = FALSE, message=FALSE}
(mle2 <- optim(par = c(0.5, 0, 1, 2, 1), fn = nllhood_mix, x = x, method = "BFGS"))
```

This means we hit our maximum number of iterations, we can start again from where we finished:

```{r, message=FALSE, warning = FALSE}
optim(par = mle2$par, fn = nllhood_mix, x = x, method = "BFGS")
```

But we still don't converge.  In this case, our likelihood rapidly decreases away from the parameter space. 

## Local minima

These starting parameters converge (with reasonable values of parameters) but not to the global minimum:
```{r, message=FALSE, warning = FALSE}
(mle3 <- optim(par = c(0.5, 1, 1, 1, 1), fn = nllhood_mix, x = x, method = "BFGS"))
mle3$par
```

Notice the value of the negative log likelihood, `r round(mle3$value, 1)`, is bigger than that at the other minimum `r round(mle1$value, 1)`.  This is a local minimum.

**Always try a few values for the starting parameters**.

## Our MLE fit

```{r}
ggplot(mapping = aes(x = x)) +
  geom_histogram(aes(y = stat(density)), binwidth = 0.25) +
  stat_function(data = data.frame(), fun = dmix, args = list(theta = mle1$par))
```

## Derivatives

We didn't supply the gradient -- `optim()` will numerically approximate it with a finite difference.

> Practical issues with general optimzation often have less to do with the optimizer than with how carefully the problem is set up.  

> In general it is worth supplying a function to calculate derivatives if you can, although it may be quicker in a one-off problem to let the software calculate numerical derivatives.

-- Ripley, B. D. "Modern applied statistics with S." Statistics and Computing, fourth ed. Springer, New York (2002).

You can do it "by hand", or for some simple functions (include things like `dnorm()`) get R to do it, see `?deriv`.

## Scaling

> It is worth ensuring the problem is reasonably well scaled, so a unit step in any parameter can have a comparable change in size to the objective, preferably about a unit change at the optimium.

-- Ripley, B. D. "Modern applied statistics with S." Statistics and Computing, fourth ed. Springer, New York (2002).



Using `nllhood()` experiment with some values of `theta`, how much does the likelihood change for a unit change in `pi`?  Compare it to a unit change in `mu_1`.

```{r, eval = FALSE}
nllhood_mix(theta = c(0.01, -0.2, 2, 2, 1), x = x)
```
```{r, eval = FALSE}
nllhood_mix(theta = c(0.09, -0.2, 2, 2, 1), x = x)
```

```{r, eval = FALSE}
nllhood_mix(theta = c(0.5, -0.2, 2, 2, 1), x = x)
```

```{r, eval = FALSE}
nllhood_mix(theta = c(0.5, 0.8, 2, 2, 1), x = x)
```

*(It will depend on the values of the other parameters, so try some near the values you would guess from the plot)*

You can specify a scaling vector with `control = list(parscale = c())`, "Optimization is performed on `par/parscale`":
```{r, warning = FALSE}
optim(par = c(0.6, -0.2, 2, 2, 1), fn = nllhood_mix, x = x, 
  method = "BFGS",
  control = list(parscale = c(5, 1, 1, 1, 1)))
```

## Bounds on parameters

Use `method = "L-BFGS-B"` and specify `lower` and `upper`.

# Other Methods

## Newton's Method (covered in the lecture)

Below is the R code for Example 2.4 in Givens, G. H. & Hoeting, J. A. (2012). *Computational Statistics*. John Wiley & Sons.

```{r}
#########################################################################
### EXAMPLE 2.4 NEWTON'S METHOD (BIVARIATE)
#########################################################################

#########################################################################
# x = initial value
# itr = number of iterations to run
# x.values = contains values of x for each iteration
# g = objective function
# g.prime = first derivative of objective function
# g.2prime = second derivative of objective function
#########################################################################

## INITIAL VALUES
x = c(-2,-2)
itr = 40
x.values = matrix(0,itr+1,2)
x.values[1,] = x

## OBJECTIVE FUNCTION AND DERIVATIVES
g = function(x){(-1)*((((x[1]^2)+x[2]-11)^2)+(x[1]+(x[2]^2)-7)^2)}

g.prime = function(x){
	g.prime.da = (-1)*((4*x[1]^3)+(4*x[1]*x[2])-(42*x[1])+(2*x[2]^2)-14)
	g.prime.db = (-1)*((2*x[1]^2)-(26*x[2])-22+(4*x[1]*x[2])+(4*x[2]^3))
	out = matrix(c(g.prime.da,g.prime.db),ncol=1)
	return(out)
}
g.2prime=function(x){
	g.2prime.da2 = (-1)*((12*x[1]^2)+(4*x[2])-42)
	g.2prime.db2 = (-1)*((12*x[2]^2)+(4*x[1])-26)
	g.2prime.dadb = (-1)*(4*(x[1]+x[2]))
	out = matrix(c(g.2prime.da2,g.2prime.dadb,
        	g.2prime.dadb,g.2prime.db2),nrow=2, byrow=TRUE)
	return(out)
}

## MAIN
for(i in 1:itr){
      x = x - solve(g.2prime(x))%*%g.prime(x)
      x.values[i+1,] = x
}

## OUTPUT
x		# FINAL ESTIMATE
g(x) 		# OBJECTIVE FUNCTION AT ESTIMATE
g.prime(x) 	# GRADIENT AT ESTIMATE

## PLOT OF CONVERGENCE
z = matrix(0,100,100)
x1.max = max(4.5,ceiling(max(x.values[,1])))
x1.min = min(-2,floor(min(x.values[,1])))
x2.max = max(3,ceiling(max(x.values[,2])))
x2.min = min(-2,floor(min(x.values[,2])))
x1 = seq(x1.min,x1.max,length=100)
x2 = seq(x2.min,x2.max,length=100)
for(i in 1:100){
      for(j in 1:100){
      	    z[i,j] = g(c(x1[i],x2[j]))
      }
}
contour(x1,x2,z,nlevels=20,drawlabels=FALSE)
for(i in 1:itr){
      segments(x.values[i,1],x.values[i,2],x.values[i+1,1],
      x.values[i+1,2],lty=2)
}
```

## Steepest Ascent (covered in the lecture)


Below is the R code for Example 2.6 in Givens, G. H. & Hoeting, J. A. (2012). *Computational Statistics*. John Wiley & Sons.

```{r}
#########################################################################
### EXAMPLE 2.6 STEEPEST ASCENT
#########################################################################

#########################################################################
# x = initial value
# M = Hessian approximation
# itr = number of iterations to run
# alpha = scale parameter
# x.values = contains values of x for each iteration
# g = objective function
# g.prime = first derivative of objective function
#########################################################################

## INITIAL VALUES
x = c(0,0)
M = diag(-1,2,2)
itr = 40
alpha.default = 1
alpha = alpha.default
x.values = matrix(0,itr+1,2)
x.values[1,] = x

## OBJECTIVE FUNCTION AND DERIVATIVES
g = function(x){(-1)*((((x[1]^2)+x[2]-11)^2)+(x[1]+(x[2]^2)-7)^2)}
g.prime = function(x){
	g.prime.da = (-1)*((4*x[1]^3)+(4*x[1]*x[2])-(42*x[1])+(2*x[2]^2)-14)
	g.prime.db = (-1)*((2*x[1]^2)-(26*x[2])-22+(4*x[1]*x[2])+(4*x[2]^3))
	out = matrix(c(g.prime.da,g.prime.db),ncol=1)
	return(out)
}

## MAIN
for (i in 1:itr){
    hessian.inv = solve(M)
    xt = x - alpha*hessian.inv%*%g.prime(x)
    # REDUCE ALPHA UNTIL A CORRECT STEP IS REACHED
    while(g(xt) < g(x)){
    		alpha = alpha/2
		xt = x - alpha*hessian.inv%*%g.prime(x)
    }
    x.values[i+1,] = x = xt
    alpha = alpha.default
}

## OUTPUT
x		# FINAL ESTIMATE
g(x) 		# OBJECTIVE FUNCTION AT ESTIMATE
g.prime(x) 	# GRADIENT AT ESTIMATE

## PLOT OF CONVERGENCE
z = matrix(0,100,100)
x1.max = max(4.5,ceiling(max(x.values[,1])))
x1.min = min(-2,floor(min(x.values[,1])))
x2.max = max(3,ceiling(max(x.values[,2])))
x2.min = min(-2,floor(min(x.values[,2])))
x1 = seq(x1.min,x1.max,length=100)
x2 = seq(x2.min,x2.max,length=100)
for(i in 1:100){
for(j in 1:100){
      z[i,j] = g(c(x1[i],x2[j]))
      }
}
contour(x1,x2,z,nlevels=20,drawlabels=FALSE)
for(i in 1:itr){
      segments(x.values[i,1],x.values[i,2],x.values[i+1,1],
      x.values[i+1,2],lty=2)
}
```

## Nelder Mead

`optim()` with default method.

No need for derivatives...can be slower to converge.

**Idea**: Evaluate function at a special arrangement of points (a simplex), then consider possible changes to the arrangement:

* Reflection
* Expansion
* Contraction
* Shrink

Nice animation: https://www.benfrederickson.com/numerical-optimization/

